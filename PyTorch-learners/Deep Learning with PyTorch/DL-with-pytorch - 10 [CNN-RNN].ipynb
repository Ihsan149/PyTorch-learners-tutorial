{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DL-with-pytorch - 10 [CNN-RNN].ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"bIAYbYajk1w9","colab_type":"text"},"cell_type":"markdown","source":["# CNN-RNN network\n","- CNNs and RNNs both have their own strengths and drawbacks. Hence, it is sometimes recommended to combine the two to model highly complicated data\n","- Here, we combine the two to classify fashion image dataset (Fashion-MNIST)"]},{"metadata":{"id":"GVU5-yp3N89I","colab_type":"code","outputId":"2a3e5076-f593-41ba-ada7-35277c683a89","executionInfo":{"status":"ok","timestamp":1545950044665,"user_tz":420,"elapsed":62689,"user":{"displayName":"Buomsoo Kim","photoUrl":"","userId":"18268696804115368229"}},"colab":{"base_uri":"https://localhost:8080/","height":336}},"cell_type":"code","source":["!pip3 install torch torchvision"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting torch\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (591.8MB)\n","\u001b[K    100% |████████████████████████████████| 591.8MB 26kB/s \n","tcmalloc: large alloc 1073750016 bytes == 0x6140c000 @  0x7efc5d2cd2a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n","\u001b[?25hCollecting torchvision\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n","\u001b[K    100% |████████████████████████████████| 61kB 20.0MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n","Collecting pillow>=4.1.1 (from torchvision)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n","\u001b[K    100% |████████████████████████████████| 2.0MB 3.3MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n","Installing collected packages: torch, pillow, torchvision\n","  Found existing installation: Pillow 4.0.0\n","    Uninstalling Pillow-4.0.0:\n","      Successfully uninstalled Pillow-4.0.0\n","Successfully installed pillow-5.3.0 torch-1.0.0 torchvision-0.2.1\n"],"name":"stdout"}]},{"metadata":{"id":"8yy37hEYOEiQ","colab_type":"code","outputId":"c35a8fa0-4744-468b-dbaf-ea28fabffe75","executionInfo":{"status":"ok","timestamp":1545968024763,"user_tz":420,"elapsed":1126,"user":{"displayName":"Buomsoo Kim","photoUrl":"","userId":"18268696804115368229"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import torch, torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","torch.__version__"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.0.0'"]},"metadata":{"tags":[]},"execution_count":1}]},{"metadata":{"id":"ewrw93tt2BfV","colab_type":"text"},"cell_type":"markdown","source":["## 1. Import & process dataset\n","- Fashion MNIST dataset from torchvision \n","- [Original dataset source](https://github.com/zalandoresearch/fashion-mnist), [paper](https://arxiv.org/abs/1708.07747)\n","\n","![](https://github.com/zalandoresearch/fashion-mnist/raw/master/doc/img/fashion-mnist-sprite.png)"]},{"metadata":{"id":"SxDHXFEsf5em","colab_type":"code","outputId":"2b71fa41-bc2e-4a89-a0a2-e9bbceca50d7","executionInfo":{"status":"ok","timestamp":1545968138956,"user_tz":420,"elapsed":5242,"user":{"displayName":"Buomsoo Kim","photoUrl":"","userId":"18268696804115368229"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["from torchvision import datasets\n","import torchvision.transforms as transforms\n","\n","train_dataset = datasets.FashionMNIST(root = \"/\", train = True, download = True, transform = transforms.ToTensor())\n","test_dataset = datasets.FashionMNIST(root = \"/\", train = False, download = True, transform = transforms.ToTensor())"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n","Processing...\n","Done!\n"],"name":"stdout"}]},{"metadata":{"id":"L5b7U5zL4fQb","colab_type":"code","colab":{}},"cell_type":"code","source":["# create data loaders \n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 128, shuffle = True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 128, shuffle = False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9nznVMdo5edZ","colab_type":"text"},"cell_type":"markdown","source":["## 2. Creating CNN-RNN model and training\n","\n","- Create and train  CNN-RNN model for fashion MNIST image classification.\n","\n","\n","![](https://www.researchgate.net/profile/Maulik_Kamdar/publication/322167103/figure/fig5/AS:631611880124465@1527599415555/CRNN-Architecture-Overview-Combining-CNN-and-RNN-to-predict-the-methylation-state-from.png)"]},{"metadata":{"id":"AQawpMRPI7jm","colab_type":"code","colab":{}},"cell_type":"code","source":["# create CNN with one convolution/pooling layer\n","class net(nn.Module):\n","  def __init__(self, input_dim, num_filters, conv_kernel_size, pool_kernel_size, stride, padding, hidden_size, num_classes, device):\n","    super(net, self).__init__()\n","    self.input_dim = input_dim\n","    self.device = device\n","    self.num_filters = num_filters\n","    self.hidden_size = hidden_size\n","    conv_output_size = int((input_dim - conv_kernel_size + 2 * padding)/stride) + 1   # conv layer output size\n","    self.pool_output_size = int((conv_output_size - pool_kernel_size)/stride) + 1          # pooling layer output size\n","   \n","    self.conv = nn.Conv2d(1, num_filters, kernel_size = conv_kernel_size, stride = stride, padding = padding)     \n","    self.pool = nn.MaxPool2d(kernel_size = pool_kernel_size, stride = stride)\n","    self.rnn = nn.GRU(input_size = self.pool_output_size * self.pool_output_size, hidden_size = hidden_size)  # GRU layer that takes into CNN output\n","    self.relu = nn.ReLU()\n","    self.dense = nn.Linear(hidden_size, num_classes)     \n","    \n","  def forward(self, x):\n","    x = self.conv(x)\n","    x = self.relu(x)\n","    x = self.pool(x)\n","\n","    x = x.view(self.num_filters, x.size(0), self.pool_output_size * self.pool_output_size)   # resize to fit into GRU layer\n","    \n","    h0 = torch.from_numpy(np.zeros((1, x.size(1), self.hidden_size))).float().to(self.device)\n","    x, _ = self.rnn(x, h0)\n","    x = x[-1, :, :]                 # take only the last sequence output\n","    x = self.dense(x)\n","    return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rP0Gt5E9ajmd","colab_type":"code","colab":{}},"cell_type":"code","source":["# hyperparameters\n","DEVICE = torch.device('cuda')\n","INPUT_DIM = 28\n","NUM_FILTERS = 64\n","HIDDEN_SIZE = 30\n","CONV_KERNEL_SIZE = 3\n","POOL_KERNEL_SIZE = 2\n","STRIDE = 1\n","PADDING = 1\n","HIDDEN_SIZE = 10\n","NUM_CLASSES = 10\n","LEARNING_RATE = 1e-1\n","NUM_EPOCHS = 10"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cPBm8qDrSWsi","colab_type":"code","colab":{}},"cell_type":"code","source":["model = net(INPUT_DIM, NUM_FILTERS, CONV_KERNEL_SIZE, POOL_KERNEL_SIZE, STRIDE, PADDING, HIDDEN_SIZE, NUM_CLASSES, DEVICE).to(DEVICE)\n","criterion = nn.CrossEntropyLoss()   # do not need softmax layer when using CEloss criterion\n","optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SEBtAPYCFeic","colab_type":"code","outputId":"69a901da-5440-441a-81cd-a71ad0993e06","executionInfo":{"status":"ok","timestamp":1545970237133,"user_tz":420,"elapsed":137066,"user":{"displayName":"Buomsoo Kim","photoUrl":"","userId":"18268696804115368229"}},"colab":{"base_uri":"https://localhost:8080/","height":193}},"cell_type":"code","source":["# training for NUM_EPOCHS\n","for i in range(NUM_EPOCHS):\n","  temp_loss = []\n","  for (x, y) in train_loader:\n","    x, y = x.float().to(DEVICE), y.to(DEVICE)  # beware that input to embedding should be type 'long'\n","    outputs = model(x)\n","    loss = criterion(outputs, y)\n","    temp_loss.append(loss.item())\n","    \n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    \n","  print(\"Loss at {}th epoch: {}\".format(i, np.mean(temp_loss)))"],"execution_count":118,"outputs":[{"output_type":"stream","text":["Loss at 0th epoch: 2.342791672200282\n","Loss at 1th epoch: 2.3466376383929872\n","Loss at 2th epoch: 2.344623132555215\n","Loss at 3th epoch: 2.3397951639537364\n","Loss at 4th epoch: 2.343589466009567\n","Loss at 5th epoch: 2.3421539873965007\n","Loss at 6th epoch: 2.3439814973233353\n","Loss at 7th epoch: 2.3448602634706477\n","Loss at 8th epoch: 2.3438526247101805\n","Loss at 9th epoch: 2.3455153802818836\n"],"name":"stdout"}]},{"metadata":{"id":"qpAJUiHm529m","colab_type":"text"},"cell_type":"markdown","source":["## 3. Evaluation\n","- Evaluate the trained CNN-RNN model with accuracy score \n","  - Store probability of each instance to a list and compare it with true y label"]},{"metadata":{"id":"txXH3dknFpSx","colab_type":"code","outputId":"ab59b91f-acab-4fc3-c7d4-c4f52a9fef51","executionInfo":{"status":"ok","timestamp":1545970889474,"user_tz":420,"elapsed":2443,"user":{"displayName":"Buomsoo Kim","photoUrl":"","userId":"18268696804115368229"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"cell_type":"code","source":["y_pred, y_true = [], []\n","with torch.no_grad():\n","  for x, y in test_loader:\n","    x, y = x.float().to(DEVICE), y.to(DEVICE)       # beware that input to embedding should be type 'long'\n","    outputs = F.softmax(model(x)).max(1)[-1]       # predicted label\n","    y_true += list(y.cpu().numpy())                # true label\n","    y_pred += list(outputs.cpu().numpy())   "],"execution_count":120,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  \"\"\"\n"],"name":"stderr"}]},{"metadata":{"id":"HV1s3xf5Frkl","colab_type":"code","outputId":"0dc389ad-cc65-4d7e-b85a-8a8e1f49f1ac","executionInfo":{"status":"ok","timestamp":1545970893107,"user_tz":420,"elapsed":748,"user":{"displayName":"Buomsoo Kim","photoUrl":"","userId":"18268696804115368229"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# evaluation result\n","from sklearn.metrics import accuracy_score\n","accuracy_score(y_true, y_pred)"],"execution_count":121,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0989"]},"metadata":{"tags":[]},"execution_count":121}]}]}