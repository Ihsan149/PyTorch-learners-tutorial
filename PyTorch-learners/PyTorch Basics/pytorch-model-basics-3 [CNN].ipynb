{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pytorch-model-basics-3 [CNN].ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"bIAYbYajk1w9","colab_type":"text"},"cell_type":"markdown","source":["# Building Blocks of Models\n","- Convolution & Pooling\n","- Padding"]},{"metadata":{"id":"GVU5-yp3N89I","colab_type":"code","outputId":"3d971b90-4a62-4564-d3a2-9259fa52b587","executionInfo":{"status":"ok","timestamp":1543983074896,"user_tz":420,"elapsed":4151,"user":{"displayName":"Buomsoo Kim","photoUrl":"","userId":"18268696804115368229"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"cell_type":"code","source":["!pip3 install torch torchvision"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.1)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.3.0)\n"],"name":"stdout"}]},{"metadata":{"id":"8yy37hEYOEiQ","colab_type":"code","outputId":"94a42058-3871-4e5f-8d1b-6a0554a3a75a","executionInfo":{"status":"ok","timestamp":1543981967794,"user_tz":420,"elapsed":1566,"user":{"displayName":"Buomsoo Kim","photoUrl":"","userId":"18268696804115368229"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import torch, torchvision\n","torch.__version__"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'0.4.1'"]},"metadata":{"tags":[]},"execution_count":2}]},{"metadata":{"id":"gyv2Sy5WO8lK","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch.nn as nn"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NAqYXyzcT-OJ","colab_type":"text"},"cell_type":"markdown","source":["## 1. Convolution & Pooling \n","Convolution and pooling are fundamental operations for building CNN models. There are a number of parameters and if their definitions are not clear, it could lead to great confusion.\n","- Parameters for convolution (pooling) layers\n","  - **stride:** how many \"steps\" that the filter makes for each advance\n","  - **kernel size**: how large is the kernel (filter) is\n","  - **number of filters (channels):** designates the \"depth\" of the data. Most image inputs have three filters (RGB)\n","  - **padding:** how to pad the input sample with zero in the border\n","- How to calculate output size of convolution/pooling operation\n","  <br> \n","*(W - F + 2P)/S + 1* <br>\n","  - *W*: input size\n","  - *F*: kernel size\n","  - *P*: padding \n","  - *S*: stride\n","  \n","![alt text](https://user-images.githubusercontent.com/22738317/34081046-c3a97518-e347-11e7-98fe-929f602ee857.png)"]},{"metadata":{"id":"XoCXfOh1RQun","colab_type":"text"},"cell_type":"markdown","source":["### 1. Convolution & Pooling 1D\n","\n","- ```torch.nn.Conv1d()```: 1D convolution\n","  - Input: (N, Fi, Li): basically, each input is ```Fi``` vectors of length ```Li```\n","    - N: batch size\n","    - Fi: number of input filters (or channels)\n","    - Li: length of input sequence\n","  - Output: (N, Fo, Lo): each output is ```Fo``` vectors of length ```Lo```\n","    - N: batch size\n","    - Fo: number of output filters (or channels)\n","    - Lo: length of output sequence"]},{"metadata":{"id":"kp_YDg0aQQ0e","colab_type":"code","outputId":"115d3bbd-055e-416b-c550-b1c9ffa45bd8","executionInfo":{"status":"ok","timestamp":1543981975275,"user_tz":420,"elapsed":1323,"user":{"displayName":"Buomsoo Kim","photoUrl":"","userId":"18268696804115368229"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# case 1 - kernel size = 1\n","conv1d = nn.Conv1d(16, 32, kernel_size = 1)\n","\n","x = torch.ones(128, 16, 10)   # input: batch_size = 128, num_filters = 16, seq_length = 10\n","print(conv1d(x).size())       # input and output size are equal when kernel_size = 1 (assuming no padding)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([128, 32, 10])\n"],"name":"stdout"}]},{"metadata":{"id":"LMcGMyJNSzSA","colab_type":"code","outputId":"5b0b68e1-6b30-4715-8693-96fc6bd724dc","executionInfo":{"status":"ok","timestamp":1538897442176,"user_tz":420,"elapsed":873,"user":{"displayName":"Buomsoo Kim","photoUrl":"","userId":"18268696804115368229"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# case 2 - kernel size = 2, stride = 1\n","conv1d = nn.Conv1d(16, 32, kernel_size = 2, padding = 2)\n","\n","x = torch.ones(128, 16, 10)   # input: batch_size = 128, num_filters = 16, seq_length = 10\n","print(conv1d(x).size())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([128, 32, 13])\n"],"name":"stdout"}]},{"metadata":{"id":"Irn1yF5lb3HO","colab_type":"code","outputId":"8a5e5025-bee4-4949-d115-63fdf9e1e70d","executionInfo":{"status":"ok","timestamp":1538897465647,"user_tz":420,"elapsed":916,"user":{"displayName":"Buomsoo Kim","photoUrl":"","userId":"18268696804115368229"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# case 2 - kernel size = 2, stride = 2\n","conv1d = nn.Conv1d(16, 64, kernel_size = 2, stride = 2, padding = 2)\n","\n","x = torch.ones(128, 16, 10)   # input: batch_size = 128, num_filters = 16, seq_length = 10\n","print(conv1d(x).size())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([128, 64, 7])\n"],"name":"stdout"}]},{"metadata":{"id":"6BF4TJ0Of1G1","colab_type":"text"},"cell_type":"markdown","source":["### Convolution & Pooling 2D\n","- ```torch.nn.Conv2d()```:  2D convolution\n","  - Largely siimilar to 1D-convolution, but input and outputs are 2-dimensional, rather than 1-dimensional\n","- Image data in Pytorch\n","  - ```Conv2d()``` is the basic building block of modern CNNs to process image, such as GoogleNet or ResNet\n","  - In Pytorch, images usually have shape of **\\[depth, height, width\\]**\n","    - depth corresponds to the number of filters (kernels)\n","    - width and height represent the size of an image. When the image is square (e.g., in CIFAR10), width = height\n","  \n","  ![alt text](http://cs231n.github.io/assets/cnn/cnn.jpeg)"]},{"metadata":{"id":"ddO_D_L2b81J","colab_type":"code","outputId":"923e57f9-8ced-4e2d-eba0-3074893cffcb","executionInfo":{"status":"ok","timestamp":1543983461231,"user_tz":420,"elapsed":1980,"user":{"displayName":"Buomsoo Kim","photoUrl":"","userId":"18268696804115368229"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["# case 1 - kernel size = 1\n","conv2d = nn.Conv2d(16, 32, kernel_size = 1)  # if kernel size is integer, width and height are equal (i.e., square kernel) \n","\n","x = torch.ones(128, 16, 10, 10)   # input: batch_size = 128, num_filters = 16, height = 10, width = 10\n","print(conv2d(x).size())       # input and output size are equal when kernel_size = 1 (assuming no padding)\n","\n","conv2d = nn.Conv2d(16, 32, kernel_size = (1, 1))  # same as kernel size = 1\n","\n","x = torch.ones(128, 16, 10, 10)   # input: batch_size = 128, num_filters = 16, height = 10, width = 10\n","print(conv2d(x).size())       # input and output size are equal when kernel_size = 1 (assuming no padding)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([128, 32, 10, 10])\n","torch.Size([128, 32, 10, 10])\n"],"name":"stdout"}]},{"metadata":{"id":"kvg4w7Q7lJBT","colab_type":"code","outputId":"1e6019f5-6756-4ad0-aa83-648f1fbd10f0","executionInfo":{"status":"ok","timestamp":1543983497981,"user_tz":420,"elapsed":1034,"user":{"displayName":"Buomsoo Kim","photoUrl":"","userId":"18268696804115368229"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# case 2 - kernel size = 2\n","conv2d = nn.Conv2d(16, 32, kernel_size = 2, padding = 1)  # if kernel size is integer, width and height are equal (i.e., square kernel) \n","\n","x = torch.ones(128, 16, 10, 10)   # input: batch_size = 128, num_filters = 16, height = 10, width = 10\n","print(conv2d(x).size())       "],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([128, 32, 11, 11])\n"],"name":"stdout"}]},{"metadata":{"id":"WMZooMlVljmy","colab_type":"code","outputId":"4f35276d-58de-4671-c6e2-4a6087216385","executionInfo":{"status":"ok","timestamp":1543983527824,"user_tz":420,"elapsed":1487,"user":{"displayName":"Buomsoo Kim","photoUrl":"","userId":"18268696804115368229"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# case 3 - differing kernel size\n","conv2d = nn.Conv2d(16, 32, kernel_size = (2, 1))  # if kernel size is integer, width and height are equal (i.e., square kernel) \n","\n","x = torch.ones(128, 16, 10, 10)   # input: batch_size = 128, num_filters = 16, height = 10, width = 10\n","print(conv2d(x).size())     # non-square output"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([128, 32, 9, 10])\n"],"name":"stdout"}]},{"metadata":{"id":"9A514b51lsxS","colab_type":"code","outputId":"f3ae5c31-ba42-41d4-b8b4-456ce43f1f81","executionInfo":{"status":"ok","timestamp":1543983641407,"user_tz":420,"elapsed":1430,"user":{"displayName":"Buomsoo Kim","photoUrl":"","userId":"18268696804115368229"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# case 4 - kernel size = 3\n","conv2d = nn.Conv2d(16, 32, kernel_size = (3, 3), padding = 1)  # if kernel size is integer, width and height are equal (i.e., square kernel) \n","\n","x = torch.ones(128, 16, 10, 10)   # input: batch_size = 128, num_filters = 16, height = 10, width = 10\n","print(conv2d(x).size())     # input and output size are equal"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([128, 32, 10, 10])\n"],"name":"stdout"}]},{"metadata":{"id":"91PuKukfmK07","colab_type":"code","outputId":"1e0560c0-49d5-4580-d6d5-0a5cde6abd2e","executionInfo":{"status":"ok","timestamp":1543983657522,"user_tz":420,"elapsed":1550,"user":{"displayName":"Buomsoo Kim","photoUrl":"","userId":"18268696804115368229"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# case 4 - kernel size = 3, stride = 2\n","conv2d = nn.Conv2d(16, 32, kernel_size = (3, 3), stride = 2)  # if kernel size is integer, width and height are equal (i.e., square kernel) \n","\n","x = torch.ones(128, 16, 10, 10)   # input: batch_size = 128, num_filters = 16, height = 10, width = 10\n","print(conv2d(x).size())     # input and output size are equal"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([128, 32, 4, 4])\n"],"name":"stdout"}]},{"metadata":{"id":"uxSuAXXRmjSJ","colab_type":"text"},"cell_type":"markdown","source":["## 2. Padding\n","- Zero padding can be applied in convolution/pooling as we have seen above. But custom padding can be applied as well\n","  - ```nn.ConstandPad1d(padding, value)```: apply constant padding on 1D data\n","    - ```padding```: the shape of padding (if tuple, (```padingLeft```, ```padingRight```))\n","    - ```value```: the value of padding\n","  - ```nn.ConstantPad2d(padding, value)```: apply constant padding on 2D data\n","    - ```padding```: the shape of padding (if tuple, (```padingLeft```, ```padingRight```, ```paddingTop```, ```padingBottom```))\n","    - ```value```: the value of padding\n","  - ```nn.ZeroPad2d(padding)```: apply zero padding on 2D data \n","    - ```padding```: the shape of padding (if tuple, (```padingLeft```, ```padingRight```, ```paddingTop```, ```padingBottom```))"]},{"metadata":{"id":"l8v3fQXooGyk","colab_type":"code","outputId":"f9380ca5-60ba-421c-c284-51b432558386","executionInfo":{"status":"ok","timestamp":1543984263422,"user_tz":420,"elapsed":1379,"user":{"displayName":"Buomsoo Kim","photoUrl":"","userId":"18268696804115368229"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["p = nn.ConstantPad1d(1, 0.75)  # 1d padding with constant 0.75\n","x = torch.ones(1, 1, 3)\n","print(p(x))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[[0.7500, 1.0000, 1.0000, 1.0000, 0.7500]]])\n"],"name":"stdout"}]},{"metadata":{"id":"QCt72l7LpNoc","colab_type":"code","outputId":"bce9e603-ef1b-4bef-9f03-70b4f3c7e2e5","executionInfo":{"status":"ok","timestamp":1543984484897,"user_tz":420,"elapsed":1077,"user":{"displayName":"Buomsoo Kim","photoUrl":"","userId":"18268696804115368229"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"cell_type":"code","source":["p = nn.ConstantPad2d((2, 2, 0, 0), -1)  # 2d padding with -1 (on right and left)\n","x = torch.ones(1, 1, 3, 3)\n","print(p(x))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[[[-1., -1.,  1.,  1.,  1., -1., -1.],\n","          [-1., -1.,  1.,  1.,  1., -1., -1.],\n","          [-1., -1.,  1.,  1.,  1., -1., -1.]]]])\n"],"name":"stdout"}]},{"metadata":{"id":"k7ucMutomgGh","colab_type":"code","outputId":"e0f5ce1a-51c2-4e53-c331-4a4172ac665d","executionInfo":{"status":"ok","timestamp":1543984158701,"user_tz":420,"elapsed":1516,"user":{"displayName":"Buomsoo Kim","photoUrl":"","userId":"18268696804115368229"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"cell_type":"code","source":["p = nn.ZeroPad2d((1,0,0,0))  # apply zero padding only on the left of first column\n","x = torch.ones(1, 1, 3, 3)\n","print(p(x))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[[[0., 1., 1., 1.],\n","          [0., 1., 1., 1.],\n","          [0., 1., 1., 1.]]]])\n"],"name":"stdout"}]}]}